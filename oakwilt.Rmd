---
title: "OakWilt"
author: "Emily Giroux"
date: "9/30/2020"
output:
  pdf_document: default
  html_document: default
urlcolor: blue
header-includes: \usepackage{xcolor}
---

```{r, global_options, eval=TRUE, echo=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff = 80), tidy = TRUE, fig.align = 'center',
               cache = FALSE, collapse = TRUE, echo = FALSE, eval = FALSE, include = FALSE,
               message = FALSE, quietly = TRUE, results = 'hide', warn.conflicts = FALSE, 
               warning = FALSE)
```

**Using package `BiocManager` to install required packages:**
```{r, biocInstall, eval=TRUE, echo=TRUE, include=TRUE}
#Installing required packages
r <- getOption("repos")
r["CRAN"] <- "http://cran.us.r-project.org"
options(repos = r)

if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

library("BiocManager")
.cran_packages <- c("data.table", "kableExtra", "knitr", "rprojroot")
.bioc_packages <- c("BiocStyle", "Biostrings", "dada2", "RAM")
.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  BiocManager::install(.bioc_packages[!.inst], ask = FALSE)
}
```
   
**Load packages into session, and print package versions:**
```{r, showBiocPackages, echo=TRUE, eval=TRUE, include=TRUE, results='hold'}
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```
**Source our custom R scripts:**    
For this we will use the rprojroot package to set the directory structures. This will help us when finding our files to source functions. We specify ours is an RStudio project. The root object contains a function that will help us locate our package R files regarless of our current working directory.
```{r sourcing_my_functions, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE}
library("rprojroot")
root        <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts     <- dir(root$find_file("R", path = root$find_file()))
scriptsl    <- paste(scriptsPath, scripts, sep = "/")
lapply(scriptsl, source)
```

Setting up working directories:
```{r}
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory"
analysis <- "oakwilt"
sharedPathAn <- paste(sharedPath, analysis, sep = "/")
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/oakwilt/"
dir.create("/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/oakwilt", 
           showWarnings = TRUE, recursive = FALSE)
baseImage <- "oakwilt.RData"
save.image(paste(imageDirPath, baseImage, sep = ""))
```

Quick image load:
```{r}
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/oakwilt/"
baseImage <- "oakwilt.RData"
load(paste(imageDirPath, baseImage, sep = ""))
```
### Step 1:       
Set up all folders (baseDir, qiime2, trimmed, logs)     
```{r}
library("data.table")
rawDataDir <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/data/forMarco"
compressedFiles <- list.files(rawDataDir, pattern = "*.bz2", full.names = TRUE)
metadata <- as.data.table(cbind(compressedFiles))
metadata$rawFileName <- basename(metadata$compressedFiles)
metadata$basename <- gsub(".tar.bz2", "", metadata$rawFileName)
rawDataWorkingPath <- paste(sharedPathAn, "rawData", sep = "/")
dir.create(rawDataWorkingPath, showWarnings = TRUE, recursive = FALSE)
metadata$rawWorkingPath <- paste(rawDataWorkingPath, metadata$basename, sep = "/")

for(i in 1:nrow(metadata)){
  cmd[i] <- paste("mkdir -p ",  rawDataWorkingPath, " && tar -xvjf ", metadata$compressedFiles[i], 
                  " -C ", rawDataWorkingPath, sep = "")  
  system(cmd[i])  
}

metadataITSF_files <- list.files(rawDataWorkingPath, pattern = "ITSF", recursive = TRUE, full.names = TRUE)
metadataITSF <- as.data.table(cbind(metadataITSF_files))
metadataITSF$basename <- basename(metadataITSF$metadataITSF_files)
metadataITSF$barcode <- gsub(".*ITS1F_", "", metadataITSF$basename)
metadataITSF$barcode <- gsub(".fastq", "", metadataITSF$barcode)
metadataITSF$barcode <- gsub("b", "B", metadataITSF$barcode)

metadataITS2R_files <- list.files(rawDataWorkingPath, pattern = "ITS2rev", recursive = TRUE, full.names = TRUE)
metadataITS2R <- as.data.table(cbind(metadataITS2R_files))
metadataITS2R$basename <- basename(metadataITS2R$metadataITS2R_files)
metadataITS2R$barcode <- gsub(".*ITS2_A_", "", metadataITS2R$basename)
metadataITS2R$barcode <- gsub(".fastq", "", metadataITS2R$barcode)

# Join the metadata samples from the forward and reverse tables using the common barcode to join rows:
setkey(metadataITSF, barcode)
setkey(metadataITS2R, barcode)

metadataITS <- merge(metadataITSF, metadataITS2R, all.x = TRUE)
setnames(metadataITS, "basename.x", "fwdFastq")
setnames(metadataITS, "basename.y", "revFastq")
metadataITS <- na.omit(metadataITS)
```

Prepare file directories:    
Qiime2     
# Input folder     
export fastq=/media/30tb_raid10/data/PIRL/2020-01-15_OAK_ITSF_30     
# Output folder     
export baseDir=/media/2TB_NVMe/pirl_2020-01-15_ITS1F     
     
        
Mimicking Marc-o's file directroy structure:    
baseDir <- sharedPathAn     
qiime2 <- paste(sharedPathAn, "qiime2", sep = "/")     
trimmed <- paste(sharedPathAn, "trimmed", sep = "/")     
logs <- paste(sharedPathAn, "logs", sep = "/")     
```{r}
# Make a directory for the trimmed data
trimmedData <- paste(sharedPathAn, "trimmed", sep = "/")
dir.create(trimmedData, showWarnings = TRUE, recursive = FALSE)

# Make a directory to hold the log files generated by itsxpress:
itsxpressLogs <- paste(sharedPathAn, "logs/itsxpress", sep = "/")
dir.create(itsxpressLogs, showWarnings = TRUE, recursive = TRUE)

qiime2Dir <- paste(sharedPathAn, "qiime2", sep = "/")
dir.create(qiime2Dir, showWarnings = TRUE, recursive = FALSE)
```

### Step 2:       
Retrieve ITS1 part of the amplicons using ITSxpress (includes trimming regions and export)      
Run ITSxpress on the raw fastq reads:   
     
**Note:** For the ITS2 region, itsxpress does not recognise the ITS2 amplicon regions and nothing is returned - no OTU table for ITS2 can be generated. Marco is looking into this issue to see if there is a sequence pattern at the ends that is inhibiting the correct processing of the sequences by itsxpress?? Possible overfitlering?? Direct checing of the sequences shows us that the ITS2 regions are in fact present, and there are many sequences observed. Problem appears to be random, and has worked with some sets of data but not others.... possible barcode issue??    
```{r}
prefix <- "ITSxpress_ITSF"
cmd <- paste("conda activate qiime2-2020.8 && itsxpress ",  
             " --fastq ", metadataITS$metadataITSF_files, 
             " --single_end ",
             " --outfile ", paste(trimmedData, "/ITSF_trimmed.", metadataITS$barcode, ".fastq", sep = ""),
             " --region ITS1 --taxa Fungi --cluster_id 0.995 ",
             " --log ", paste(itsxpressLogs, "/ITSF_trimmed.", metadataITS$barcode, ".log", sep = ""),
             " && conda deactivate ",
             sep = "")  
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Running itsxpress on the itsf b56 reads separately, since they do not have a pair with the reverse reads for this barcode and are therefore not in the final metadataITS table, maybe they should be there?
```{r}
cmd <- paste("itsxpress ",  
             " --fastq ", metadataITSF$metadataITSF_files[56], 
             " --single_end ",
             " --outfile ", paste(trimmedData, "/ITSF_trimmed.", metadataITSF$barcode[56], ".fastq", sep = ""),
             " --region ITS1 --taxa Fungi --cluster_id 0.995 ",
             " --log ", paste(itsxpressLogs, "/ITSF_trimmed.", metadataITSF$barcode[56], ".log", sep = ""),
             sep = "")  
# Run the above command directly on the command line, since it's just for one file and shouldn't take long.
```

Add the path to the trimmed fastq files and a column to set unique sample names based on the filename/sequencing run and sample barcode number:
```{r}
metadataITSF$trimmedPath <- paste(trimmedData, "/ITSF_trimmed.", metadataITSF$barcode, ".fastq", sep = "")
metadataITSF$sampleID <- paste("ITSF_OAK_2019Plate1", metadataITSF$barcode, sep = "_")
```

Create a manifest file that qiime2 will use to import our fastq data and write it to a tsv file:
```{r}
library("data.table")
manifest <- metadataITSF[, .('sample-id' = sampleID, 'absolute-filepath' = trimmedPath)]

write.table(manifest, file = paste(sharedPathAn, "qiime2_import_manifest.tsv", sep = "/"), 
            quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
```

# import fastq files
qiime tools import \
    --type 'SampleData[SequencesWithQuality]' \
    --input-path "${baseDir}"/fastq \
    --output-path "${baseDir}"/qiime2/demux-single-end.qza \
    --input-format CasavaOneEightSingleLanePerSampleDirFmt
```{r}
prefix <- "qiimeImport"
cmd <- paste("conda activate qiime2-2020.8 && ",
             "qiime tools import ",
             " --type 'SampleData[SequencesWithQuality]' ",
             " --input-path ", paste(sharedPathAn, "qiime2_import_manifest.tsv", sep = "/"),
             " --output-path ", paste(qiime2Dir, "/demux-single-end.qza", sep = ""),
             " --input-format SingleEndFastqManifestPhred33V2 ",
             " && conda deactivate ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
Create a file to visualise the qiime2 fastq files imported:
```{r}
prefix <- "qiimeVisualiseImport"
cmd <- paste("conda activate qiime2-2020.8 && ",
             " qiime demux summarize ",
             " --i-data  ", paste(qiime2Dir, "/demux-single-end.qza", sep = ""),
             " --o-visualization ", paste(qiime2Dir, "/demux-single-end.qzv", sep = ""),
             " --verbose ", 
             " && conda deactivate ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
To view demux-single-end.qzv, open https://view.qiime2.org/ with your browser and drag the file into the window at the top of the page.     
    
**To remove the output files after you are done:**
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Denoise the sequences with dada2 within qiime2:    
- corrects sequencing errors    
- removes chimeras    
- clusters sequences at 100% similarity    
- outputs an asv table and the representative sequences
```{r}
prefix <- "qiimeDADA2deNoiseSingle"
cmd <- paste("conda activate qiime2-2020.8 && ",
             " qiime dada2 denoise-single ",
             " --i-demultiplexed-seqs ", paste(qiime2Dir, "/demux-single-end.qza", sep = ""),
             " --p-trim-left 0 ",
             " --p-trunc-len 0 ",
             " --o-representative-sequences ", paste(qiime2Dir, "/rep-seqs-dada2.qza", sep = ""),
             " --o-table ", paste(qiime2Dir, "/table-dada2.qza", sep = ""),
             " --o-denoising-stats ", paste(qiime2Dir, "/stats-dada2.qza", sep = ""),
             " --p-n-threads 20 ", 
             " --verbose ", 
             " && conda deactivate ", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
Export the dada2 results:    
# Export OTU table
mkdir phyloseq
qiime tools export \
--input-path table.qza \
--output-path phyloseq

# Convert biom format to tsv format
biom convert \
-i phyloseq/feature-table.biom \
-o phyloseq/otu_table.tsv \
--to-tsv
cd phyloseq
sed -i '1d' otu_table.tsv
sed -i 's/#OTU ID//' otu_table.tsv
cd ../

# Export representative sequences
qiime tools export \
--input-path rep-seqs.qza \
--output-path phyloseq
```{r}




```



Run dada2 on files, use Marco's script and modify for here:
```{r}
library("dada2")

# Unzipped fastq files
# path <- '/media/2TB_NVMe/pirl_2020-01-15_ITS1F/tmp'

# fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
# sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# Filter and trim
# filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
# names(filtFs) <- sample.names
# out <- filterAndTrim(fnFs, filtFs, maxN=0, maxEE=2, truncQ=2, minLen=50, 
#                      compress=TRUE, multithread=TRUE)

# Learn the Error Rates
# errF <- learnErrors(filtFs, multithread=TRUE)

# Sample Inference
# dadaFs <- dada(filtFs, err=errF, multithread=TRUE,
#                HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32)

# Construct an amplicon sequence variant table (ASV) table
# seqtab <- makeSequenceTable(dadaFs)

# Track reads through the pipeline
# getN <- function(x) sum(getUniques(x))
# track <- cbind(out, sapply(dadaFs, getN))
# colnames(track) <- c("input", "filtered", "denoisedF")
# rownames(track) <- sample.names
# track

# Assign taxonomy
# taxa <- assignTaxonomy(seqtab, "/media/30tb_raid10/db/UNITE/fasta/UNITE_82.fasta", multithread=TRUE)
# Phyloseq
```


### Step 5:       
Run RAM       


















library("RAM")
library("data.table")


Testing run of kmc:
```{r}
prefix <- "kmc_inclusion"
cmd <- paste("conda activate insilicoPrimer && cd ", sharedPathAn, 
             " && kmc -k99 -v @", 
             inclusionPathList, " inclusion ", " . ", 
             " && conda deactivate", sep = "")
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```